<!DOCTYPE html><html lang="ja"> <head><meta charset="utf-8"><title>20250430</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head> <body> <header><a href="/">Home</a></header> <main>  <article> <h1 id="20250430">2025/04/30</h1>
<h2 id="facebook-parent-meta-platforms-launches-standalone-ai-assistant-app">Facebook parent Meta Platforms launches standalone AI assistant app</h2>
<h3 id="リンク">リンク</h3>
<ul>
<li><a href="https://www.reuters.com/business/facebook-parent-meta-platforms-launches-standalone-ai-assistant-app-2025-04-29/">https://www.reuters.com/business/facebook-parent-meta-platforms-launches-standalone-ai-assistant-app-2025-04-29/</a></li>
</ul>
<h3 id="英語要約">英語要約</h3>
<p>Meta released a <strong>stand-alone “Meta AI” app</strong> that runs on its latest multimodal Llama 4 model.<br>
The voice-first app offers full-duplex (simultaneous listen / speak) conversations, can — with permission — draw context from Facebook and Instagram data for personalized answers, and hands off chats to Meta’s smart-glasses. The move, announced at Meta’s first “LlamaCon,” positions the company to compete head-to-head with OpenAI’s ChatGPT and Google’s Gemini assistants. (<a href="https://www.reuters.com/business/facebook-parent-meta-platforms-launches-standalone-ai-assistant-app-2025-04-29/?utm_source=chatgpt.com">Facebook parent Meta Platforms launches standalone AI assistant app</a>)</p>
<h3 id="日本語翻訳">日本語翻訳</h3>
<p>Meta は最新のマルチモーダル LLM「Llama 4」を搭載した**単独アプリ“Meta AI”**を公開しました。<br>
音声主体のフルデュープレックス対話が可能で、ユーザーが許可すれば Facebook／Instagram のデータを参照して回答をパーソナライズし、会話を Meta 製スマートグラスへも引き継げます。初開催の「LlamaCon」で発表され、OpenAI や Google の音声アシスタントに真正面から挑む布陣です。 (<a href="https://www.reuters.com/business/facebook-parent-meta-platforms-launches-standalone-ai-assistant-app-2025-04-29/?utm_source=chatgpt.com">Facebook parent Meta Platforms launches standalone AI assistant app</a>)</p>
<h3 id="感想">感想</h3>
<p>Llama 規模のモデルを作成するとなると、莫大な資金が必要なはず。 Meta は Llama を OSS で作成してくれていて、Meta には LLM で稼いで欲しいところ。
スマートグラスと連携できるみたいだし、Meta の目指すビジョンと関連して良いおとしどころを見つけたのかもしれない。</p>
<h2 id="akamai-firewall-for-ai-enables-secure-ai-applications-with-advanced-threat-protection">Akamai Firewall for AI Enables Secure AI Applications with Advanced Threat Protection</h2>
<h3 id="リンク-1">リンク</h3>
<ul>
<li><a href="https://www.akamai.com/newsroom/press-release/akamai-firewall-for-ai-enables-secure-ai-applications-with-advanced-threat-protection">https://www.akamai.com/newsroom/press-release/akamai-firewall-for-ai-enables-secure-ai-applications-with-advanced-threat-protection</a></li>
</ul>
<h3 id="英語要約-1">英語要約</h3>
<p>Akamai introduced <strong>“Firewall for AI,”</strong> an edge service that shields LLM-based apps from prompt-injection, model-extraction, data-scraping, and toxic-output risks.<br>
It also debuts <strong>API LLM Discovery</strong> to auto-find GenAI endpoints and apply adaptive security policies, giving enterprises a single layer of defense for both inbound prompts and outbound responses. (<a href="https://www.akamai.com/newsroom/press-release/akamai-firewall-for-ai-enables-secure-ai-applications-with-advanced-threat-protection">Akamai Firewall for AI Enables Secure AI Applications with Advanced Threat Protection | Akamai</a>)</p>
<h3 id="日本語翻訳-1">日本語翻訳</h3>
<p>Akamai は生成 AI 向けの新製品**「Firewall for AI」**を発表しました。<br>
LLM へのプロンプトインジェクションやモデル抽出、大規模スクレイピング、トキシック出力をブロックし、<strong>API LLM Discovery</strong>がエンドポイントを自動検出して動的ポリシーを適用。入力と出力の双方を保護する包括的な防御レイヤーを提供します。 (<a href="https://www.akamai.com/newsroom/press-release/akamai-firewall-for-ai-enables-secure-ai-applications-with-advanced-threat-protection">Akamai Firewall for AI Enables Secure AI Applications with Advanced Threat Protection | Akamai</a>)</p>
<h3 id="感想-1">感想</h3>
<p>LLM 使ったサービスは既存の Web アプリに対する脆弱性対策に加えて、LLM 特有の仕組みも必要になってくるんだろう。
仕事でそういう LLM 系のサービスを作る機会が多いし使ってみたい。</p>
<h2 id="writer-releases-palmyra-x5-delivers-near-gpt-41-performance-at-75--lower-cost">Writer releases Palmyra X5, delivers near GPT-4.1 performance at 75 % lower cost</h2>
<h3 id="リンク-2">リンク</h3>
<ul>
<li><a href="https://venturebeat.com/ai/writer-releases-palmyra-x5-delivers-near-gpt-4-performance-at-75-lower-cost/">https://venturebeat.com/ai/writer-releases-palmyra-x5-delivers-near-gpt-4-performance-at-75-lower-cost/</a></li>
</ul>
<h3 id="英語要約-2">英語要約</h3>
<p>Enterprise-AI firm <strong>Writer</strong> unveiled <strong>Palmyra X5</strong>, a Mixture-of-Experts LLM with a <strong>1 million-token context window</strong>.<br>
Priced at $0.60 per million input tokens, it ingests a full 1 M-token prompt in ~22 s and returns responses in fractions of a second—offering near GPT-4.1 quality at roughly 75 % lower cost, aimed squarely at long-context AI agents. (<a href="https://venturebeat.com/ai/writer-releases-palmyra-x5-delivers-near-gpt-4-performance-at-75-lower-cost/">Writer releases Palmyra X5, delivers near GPT-4.1 performance at 75% lower cost | VentureBeat</a>)</p>
<h3 id="日本語翻訳-2">日本語翻訳</h3>
<p>エンタープライズ向け生成 AI 企業 Writer は、<strong>100 万トークン文脈窓</strong>を持つ LLM **「Palmyra X5」**を発表しました。<br>
入力 100 万トークン当たり 0.60 ドルという価格で、巨大プロンプトを約 22 秒で読み込み、サブ秒で応答を返します。GPT-4.1 並みの性能を約 75 %低コストで実現し、長文脈を扱う AI エージェント用途を狙います。 (<a href="https://venturebeat.com/ai/writer-releases-palmyra-x5-delivers-near-gpt-4-performance-at-75-lower-cost/">Writer releases Palmyra X5, delivers near GPT-4.1 performance at 75% lower cost | VentureBeat</a>)</p>
<h3 id="感想-2">感想</h3>
<p>ここら辺の大型モデルの API 競争は激化していくだろう。
とはいえ、このモデルが本当に GPT-4.1 並みの性能を持っているのかは、実際に使ってみないとわからない。
LLM って細かい FineTune だったりがユーザビリティに影響する気がするので、そこら辺を OpenAI や Anthoropic と同程度にできているかは気になるところ。</p>
<h2 id="announcing-the-general-availability-of-llama-4-maas-on-vertex-ai">Announcing the general availability of Llama 4 MaaS on Vertex AI</h2>
<h3 id="リンク-3">リンク</h3>
<ul>
<li><a href="https://developers.googleblog.com/en/llama-4-ga-maas-vertex-ai/">https://developers.googleblog.com/en/llama-4-ga-maas-vertex-ai/</a></li>
</ul>
<h3 id="英語要約-3">英語要約</h3>
<p>Google Cloud made <strong>Meta’s Llama 4</strong> generally available as a fully-managed “Model-as-a-Service” on Vertex AI.<br>
Developers can invoke Llama 4 (and a new Llama 3.3 70B) via the Vertex Model Garden without managing infrastructure, benefiting from enterprise-grade security, predictable pricing, and provisioned throughput options. ([</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>        Announcing the general availability of Llama 4 MaaS on Vertex AI</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>        - Google Developers Blog</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    ](https://developers.googleblog.com/en/llama-4-ga-maas-vertex-ai/))</span></span></code></pre>
<h3 id="日本語翻訳-3">日本語翻訳</h3>
<p>Google Cloud は**「Llama 4 MaaS」<strong>を Vertex AI で一般提供開始し、インフラ管理なしで高度な推論・画像理解機能を利用可能にしました。<br>
同時に</strong>Llama 3.3 70B**も提供し、エンタープライズ向けのセキュリティや従量課金・専有スループットオプションを備えています。 ([</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>        Announcing the general availability of Llama 4 MaaS on Vertex AI</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>        - Google Developers Blog</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    ](https://developers.googleblog.com/en/llama-4-ga-maas-vertex-ai/))</span></span></code></pre>
<h3 id="感想-3">感想</h3>
<p>結局、Vertex AI、Azure OpenAI Service、Amazon Bedrock のどれを使えば良いんだろうね。</p>
<h2 id="hikvision-unveils-guanlan-large-scale-ai-models-to-power-next-gen-aiot-applications">Hikvision unveils Guanlan Large-Scale AI Models to power next-gen AIoT applications</h2>
<h3 id="リンク-4">リンク</h3>
<ul>
<li><a href="https://www.hikvision.com/en/newsroom/latest-news/2025/hikvision-unveils-guanlan-large-scale-ai-models-to-power-next-gen-aiot-products-and-applications/">https://www.hikvision.com/en/newsroom/latest-news/2025/hikvision-unveils-guanlan-large-scale-ai-models-to-power-next-gen-aiot-products-and-applications/</a></li>
</ul>
<h3 id="英語要約-4">英語要約</h3>
<p>Hikvision launched <strong>“Guanlan” Large-Scale AI Models</strong>, a three-tier stack of vision, language, and multimodal foundation models topped with industry-specific and task-specific layers to drive scenario-based AIoT solutions such as smart cameras and natural-language video search. (<a href="https://www.hikvision.com/en/newsroom/latest-news/2025/hikvision-unveils-guanlan-large-scale-ai-models-to-power-next-gen-aiot-products-and-applications/">Hikvision unveils Guanlan Large-Scale AI Models to power next-gen AIoT products and applications</a>)</p>
<h3 id="日本語翻訳-4">日本語翻訳</h3>
<p>Hikvision は、画像・言語・マルチモーダル基盤モデルを核とし、その上に業界別・タスク別モデルを重ねた三層構成の**大型 AI モデル群「Guanlan」**を発表。スマートカメラや自然言語による映像検索など、AIoT 向けシナリオデジタル化を加速します。 (<a href="https://www.hikvision.com/en/newsroom/latest-news/2025/hikvision-unveils-guanlan-large-scale-ai-models-to-power-next-gen-aiot-products-and-applications/">Hikvision unveils Guanlan Large-Scale AI Models to power next-gen AIoT products and applications</a>)</p>
<h3 id="感想-4">感想</h3>
<p>toB 向けだから個人では使えなさそう？
実際に AIoT というのがどの程度浸透するか。</p> </article>  </main> <footer>© 2025 justy's blog</footer> </body></html>