<!DOCTYPE html><html lang="ja"> <head><meta charset="utf-8"><title>20250507</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head> <body> <header><a href="/">Home</a></header> <main>  <article> <h1 id="20250507">2025/05/07</h1>
<h2 id="servicenow-launches-new-ai-agent-solutions">ServiceNow launches new AI agent solutions</h2>
<h3 id="リンク">リンク</h3>
<ul>
<li><a href="https://www.arnnet.com.au/article/3978969/servicenow-launches-new-ai-agent-solutions.html">https://www.arnnet.com.au/article/3978969/servicenow-launches-new-ai-agent-solutions.html</a></li>
</ul>
<h3 id="英語要約">英語要約</h3>
<p>ServiceNow used its Knowledge 2025 conference to unveil <strong>AI Control Tower</strong> and <strong>AI Agent Fabric</strong>, positioning them as an enterprise operating layer for the coming “agentic” era. Control Tower gives companies a single cockpit to observe, govern, and secure any AI agent or model—whether ServiceNow-native or third-party (GPT-4, Gemini, Llama 3, etc.). Agent Fabric, meanwhile, lets multiple agents and multimodal models collaborate on complex workflows, orchestrating hand-offs across systems. Together they promise policy-compliant oversight, real-time ROI dashboards, and faster deployment of specialized AI agents.</p>
<h3 id="日本語翻訳">日本語翻訳</h3>
<p>ServiceNow は年次イベント <em>Knowledge 2025</em> で <strong>AI Control Tower</strong> と <strong>AI Agent Fabric</strong> を発表し、エージェント時代の「企業 OS」として位置づけた。Control Tower は、ServiceNow 製か外部製かを問わず、あらゆる AI エージェント／モデルを一元可視化し、統治・セキュリティ・ROI 追跡を行うコックピットとなる。一方、Agent Fabric は複数エージェントやマルチモーダルモデル間の連携を可能にし、複雑なワークフローを横断的に編成する。両者を併せることで、ポリシー準拠の運用監視、リアルタイムな価値測定、専門エージェントの迅速な展開を実現する。</p>
<h3 id="感想">感想</h3>
<p>ServiceNow は ServiceNow というアプリケーションを販売しているっぽい。自社のデータセンターにクラウドとして提供してるっぽい。
会社で起こること(IT, 人事, 法務, 顧客対応など)を一元管理して、アプリケーションで業務を支援する。
<a href="https://qiita.com/Nanase_Ariyama/items/5cb936bfddad3d383e76">参考</a></p>
<p>そういう企業が Agent を提供するという話。当たり前の流れな気がするよね。</p>
<h2 id="google-improves-coding-capabilities-in-gemini-25-pro-preview">Google Improves Coding Capabilities in Gemini 2.5 Pro Preview</h2>
<h3 id="リンク-1">リンク</h3>
<ul>
<li><a href="https://www.pymnts.com/artificial-intelligence-2/2025/google-improves-coding-capabilities-in-gemini-2-5-pro-preview/">https://www.pymnts.com/artificial-intelligence-2/2025/google-improves-coding-capabilities-in-gemini-2-5-pro-preview/</a></li>
</ul>
<h3 id="英語要約-1">英語要約</h3>
<p>Google released an early preview of <strong>Gemini 2.5 Pro</strong>, touting dramatically improved web-app coding, a leading score on the LM-Arena WebDev benchmark, and multimodal prowess (VideoMME 84.8 %). Context length will expand toward <strong>2 million tokens</strong>.</p>
<h3 id="日本語翻訳-1">日本語翻訳</h3>
<p>Google は <strong>Gemini 2.5 Pro</strong> のプレビュー版を公開。Web アプリ生成に特化したコード生成が大幅向上し、LM-Arena WebDev ベンチで首位を獲得。VideoMME 84.8 % を記録するなどマルチモーダル性能も強化。コンテキスト長は <strong>200 万トークン</strong> へ拡張予定。</p>
<h3 id="感想-1">感想</h3>
<p>そもそも Gemini 2.5 Pro っていつ出たっけって調べたら<a href="https://x.com/Google/status/1904579652816941506">2025 年 3 月 26 日っぽい</a>
コード生成系だと Claude に劣る印象があるけど、それを払拭できるか？</p>
<h2 id="multimodal-ai-makes-this-robot-vacuum-smart-enough-to-pick-up-your-socks">Multimodal AI makes this robot vacuum smart enough to pick up your socks</h2>
<h3 id="リンク-2">リンク</h3>
<ul>
<li><a href="https://www.businessinsider.com/sc/ai-powered-robot-vacuum-that-will-transform-your-home-life">https://www.businessinsider.com/sc/ai-powered-robot-vacuum-that-will-transform-your-home-life</a></li>
</ul>
<h3 id="英語要約-2">英語要約</h3>
<p>Roborock’s forthcoming <strong>Saros Z70</strong> robot vacuum integrates a foldable 5-axis arm plus 3D LiDAR, RGB, ToF, and IR sensors, all fused by multimodal AI. It can identify small objects (≥2 cm²) and tidy them before vacuuming—while remaining only 7.98 cm tall. OTA updates extend object vocabulary over time.</p>
<h3 id="日本語翻訳-2">日本語翻訳</h3>
<p>Roborock の新型掃除ロボ <strong>Saros Z70</strong> は 5 軸ロボットアームと 3D LiDAR／RGB／ToF／IR センサーを統合したマルチモーダル AI を搭載。2 cm² 以上の小物を識別し、掃除前に片付ける。高さ 7.98 cm の薄型設計で、OTA アップデートにより認識対象が拡張される。</p>
<h3 id="感想-2">感想</h3>
<p>これって推論はどこでやってるんだろう。ロボットの中でやってるのか、API 投げてるのか。気になるな。</p>
<h2 id="perception-language-models-plms-by-meta-a-breakthrough-in-open-visual-understanding">Perception Language Models (PLMs) by Meta: A Breakthrough in Open Visual Understanding</h2>
<h3 id="リンク-3">リンク</h3>
<ul>
<li><a href="https://medium.com/%40ashishchadha11944/perception-language-models-plms-by-meta-a-breakthrough-in-open-visual-understanding-f6cc3259976b">https://medium.com/%40ashishchadha11944/perception-language-models-plms-by-meta-a-breakthrough-in-open-visual-understanding-f6cc3259976b</a></li>
</ul>
<h3 id="英語要約-3">英語要約</h3>
<p>Meta released <strong>PLM-1B/3B/8B</strong> along with 2.8 million human-labeled video QA pairs and the PLM-VideoBench evaluation suite. The models couple a new perception encoder with Llama 3 decoders via an MLP bridge, delivering open, reproducible vision–language research without distillation from proprietary systems.</p>
<h3 id="日本語翻訳-3">日本語翻訳</h3>
<p>Meta は <strong>PLM-1B/3B/8B</strong> を公開し、280 万件の人手ラベル付き動画 QA と PLM-VideoBench を同時リリース。独自の Perception Encoder と Llama 3 デコーダを MLP で接続し、クローズドモデルの蒸留に頼らない再現性重視の VLM 研究基盤を提供する。</p>
<h3 id="感想-3">感想</h3>
<p>PLM というのは VLM と何が違うのか。ちゃんと調査する。</p> </article>  </main> <footer>© 2025 justy's blog</footer> </body></html>